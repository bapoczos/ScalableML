{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Drabas & Lee  -- Learning PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the jupyter notebook from its own folder, otherwise python might not find some files to load!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the kernel to python 2 or Python [default]!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you only need to run this cell if the above spark context is not available when you start the notebook\n",
    "\n",
    "if 0:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    import pyspark\n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to create an RDD in PySpark. 1) You can parallelize a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(\n",
    "    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), \n",
    "     ('Amber', 9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or 2) read from a repository (a file or a database). \n",
    "We will discuss it below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are schema-less data structures (unlike DataFrames, which we will discuss later). Thus, parallelizing a dataset, such as in the following code snippet, \n",
    "is perfectly fine with Spark when using RDDs. We can mix amost everything: tuple, dict, list, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:195\n"
     ]
    }
   ],
   "source": [
    "data_heterogenous = sc.parallelize([('Ferrari', 'fast'), {'Porsche': 100000}, ['Spain','visited', 4504]])\n",
    "print(data_heterogenous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ferrari', 'fast'), {'Porsche': 100000}, ['Spain', 'visited', 4504]]\n"
     ]
    }
   ],
   "source": [
    "data_heterogenous_collected = data_heterogenous.collect() \n",
    "print(data_heterogenous_collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .collect() method returns all the elements of the RDD to the driver where it is serialized as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the data in the object as you would normally do in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous_collected[1]['Porsche']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spain', 'visited', 4504]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous_collected[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ferrari', 'fast')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_heterogenous_collected[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that to execute the code above you will have to change the path where the data is stored. The dataset can be downloaded from http://tomdrabas.com/data/VS14MORT.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/Projects/ScalableML/PySparkDemos\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!wget http://tomdrabas.com/data/VS14MORT.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix hdfs if files are corrupt...\n",
    "#\n",
    "#!hdfs fsck -list-corruptfileblocks / \n",
    "#!hdfs dfsadmin -safemode leave\n",
    "#!hdfs dfs -rm /hdfs_data/VS14MORT.txt.gz\n",
    "#!hdfs dfs -rm /hdfs_data/*\n",
    "#!hdfs dfs -rm -r /user/ec2-user/data_key*\n",
    "#!hdfs fsck /\n",
    "#!hdfs fsck / -delete\n",
    "#!hdfs fsck /hdfs_data/ -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 ec2-user supergroup   93093001 2019-02-10 23:57 /hdfs_data/VS14MORT.txt.gz\n",
      "put: `/hdfs_data/VS14MORT.txt.gz': File exists\n",
      "Connecting to namenode via http://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:50070/fsck?ugi=ec2-user&path=%2Fhdfs_data%2FVS14MORT.txt.gz\n",
      "FSCK started by ec2-user (auth:SIMPLE) from /172.31.5.183 for path /hdfs_data/VS14MORT.txt.gz at Mon Feb 11 00:17:22 UTC 2019\n",
      ".\n",
      "/hdfs_data/VS14MORT.txt.gz:  Under replicated BP-663532545-172.31.27.125-1549216637007:blk_1073741830_1006. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s) and 0 decommissioning replica(s).\n",
      "Status: HEALTHY\n",
      " Total size:\t93093001 B\n",
      " Total dirs:\t0\n",
      " Total files:\t1\n",
      " Total symlinks:\t\t0\n",
      " Total blocks (validated):\t1 (avg. block size 93093001 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t1 (100.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t2.0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t1 (33.333332 %)\n",
      " Number of data-nodes:\t\t2\n",
      " Number of racks:\t\t1\n",
      "FSCK ended at Mon Feb 11 00:17:22 UTC 2019 in 0 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/hdfs_data/VS14MORT.txt.gz' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /hdfs_data\n",
    "!hdfs dfs -ls /hdfs_data\n",
    "!hdfs dfs -put data/VS14MORT.txt.gz /hdfs_data\n",
    "!hdfs fsck /hdfs_data/VS14MORT.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hdfs_data/VS14MORT.txt.gz MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    data_from_file_long = sc.textFile('data/VS14MORT.txt.gz', 4)\n",
    "else:\n",
    "    data_from_file_long = sc.textFile('/hdfs_data/VS14MORT.txt.gz', 4)\n",
    "print(data_from_file_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last parameter in sc.textFile(..., n) specifies the number of partitions the dataset is divided into. <br>\n",
    "\n",
    "Spark can read from a multitude of filesystems: Local ones such as NTFS, FAT, or Mac OS Extended (HFS+), or distributed filesystems such as HDFS, S3, Cassandra, among many others <br>\n",
    "\n",
    "Note that Spark can automatically work with compressed datasets (like the Gzipped one in our preceding example). <br>\n",
    "\n",
    "Depending on how the data is read, the object holding it will be represented slightly  differently. The data read from a file is represented as MapPartitionsRDD instead  of ParallelCollectionRDD when we .paralellize(...) a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to make the computations quicker in this demo, let us make the RDD smaller ...\n",
    "data_from_file=data_from_file_long.sample(False, 0.0001,345)\n",
    "data_from_file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you read from a text file, each row from the file forms an element of an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'                   2                                        00 002  F4001 030101031S7                2014U7UN                                    R571380 11013636 0111R571                                                                                                                                                                          01 R571                                                                                                 01  11                                 100 601']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'                   2                                        00 002  F4001 030101031S7                2014U7UN                                    R571380 11013636 0111R571                                                                                                                                                                          01 R571                                                                                                 01  11                                 100 601',\n",
       " u'                   1                                        06 006  F1065 391909  2M5                2014U7UN                                    E780173 111   37 0311I469 21I519 31E780                                                                                                                                                            03 E780 I469 I519                                                                                       02  32                                 100 702']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more readable, let's create a list of elements so each line is represented as a list of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define the method with the help of the following code, which will parse the unreadable row into something that we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractInformation(row):\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    selected_indices = [\n",
    "         2,4,5,6,7,9,10,11,12,13,14,15,16,17,18,\n",
    "         19,21,22,23,24,25,27,28,29,30,32,33,34,\n",
    "         36,37,38,39,40,41,42,43,44,45,46,47,48,\n",
    "         49,50,51,52,53,54,55,56,58,60,61,62,63,\n",
    "         64,65,66,67,68,69,70,71,72,73,74,75,76,\n",
    "         77,78,79,81,82,83,84,85,87,89\n",
    "    ]\n",
    "\n",
    "    '''\n",
    "        Input record schema\n",
    "        schema: n-m (o) -- xxx\n",
    "            n - position from\n",
    "            m - position to\n",
    "            o - number of characters\n",
    "            xxx - description\n",
    "        1. 1-19 (19) -- reserved positions\n",
    "        2. 20 (1) -- resident status\n",
    "        3. 21-60 (40) -- reserved positions\n",
    "        4. 61-62 (2) -- education code (1989 revision)\n",
    "        5. 63 (1) -- education code (2003 revision)\n",
    "        6. 64 (1) -- education reporting flag\n",
    "        7. 65-66 (2) -- month of death\n",
    "        8. 67-68 (2) -- reserved positions\n",
    "        9. 69 (1) -- sex\n",
    "        10. 70 (1) -- age: 1-years, 2-months, 4-days, 5-hours, 6-minutes, 9-not stated\n",
    "        11. 71-73 (3) -- number of units (years, months etc)\n",
    "        12. 74 (1) -- age substitution flag (if the age reported in positions 70-74 is calculated using dates of birth and death)\n",
    "        13. 75-76 (2) -- age recoded into 52 categories\n",
    "        14. 77-78 (2) -- age recoded into 27 categories\n",
    "        15. 79-80 (2) -- age recoded into 12 categories\n",
    "        16. 81-82 (2) -- infant age recoded into 22 categories\n",
    "        17. 83 (1) -- place of death\n",
    "        18. 84 (1) -- marital status\n",
    "        19. 85 (1) -- day of the week of death\n",
    "        20. 86-101 (16) -- reserved positions\n",
    "        21. 102-105 (4) -- current year\n",
    "        22. 106 (1) -- injury at work\n",
    "        23. 107 (1) -- manner of death\n",
    "        24. 108 (1) -- manner of disposition\n",
    "        25. 109 (1) -- autopsy\n",
    "        26. 110-143 (34) -- reserved positions\n",
    "        27. 144 (1) -- activity code\n",
    "        28. 145 (1) -- place of injury\n",
    "        29. 146-149 (4) -- ICD code\n",
    "        30. 150-152 (3) -- 358 cause recode\n",
    "        31. 153 (1) -- reserved position\n",
    "        32. 154-156 (3) -- 113 cause recode\n",
    "        33. 157-159 (3) -- 130 infant cause recode\n",
    "        34. 160-161 (2) -- 39 cause recode\n",
    "        35. 162 (1) -- reserved position\n",
    "        36. 163-164 (2) -- number of entity-axis conditions\n",
    "        37-56. 165-304 (140) -- list of up to 20 conditions\n",
    "        57. 305-340 (36) -- reserved positions\n",
    "        58. 341-342 (2) -- number of record axis conditions\n",
    "        59. 343 (1) -- reserved position\n",
    "        60-79. 344-443 (100) -- record axis conditions\n",
    "        80. 444 (1) -- reserve position\n",
    "        81. 445-446 (2) -- race\n",
    "        82. 447 (1) -- bridged race flag\n",
    "        83. 448 (1) -- race imputation flag\n",
    "        84. 449 (1) -- race recode (3 categories)\n",
    "        85. 450 (1) -- race recode (5 categories)\n",
    "        86. 461-483 (33) -- reserved positions\n",
    "        87. 484-486 (3) -- Hispanic origin\n",
    "        88. 487 (1) -- reserved\n",
    "        89. 488 (1) -- Hispanic origin/race recode\n",
    "     '''\n",
    "\n",
    "    record_split = re\\\n",
    "        .compile(\n",
    "            r'([\\s]{19})([0-9]{1})([\\s]{40})([0-9\\s]{2})([0-9\\s]{1})([0-9]{1})([0-9]{2})' + \n",
    "            r'([\\s]{2})([FM]{1})([0-9]{1})([0-9]{3})([0-9\\s]{1})([0-9]{2})([0-9]{2})' + \n",
    "            r'([0-9]{2})([0-9\\s]{2})([0-9]{1})([SMWDU]{1})([0-9]{1})([\\s]{16})([0-9]{4})' +\n",
    "            r'([YNU]{1})([0-9\\s]{1})([BCOU]{1})([YNU]{1})([\\s]{34})([0-9\\s]{1})([0-9\\s]{1})' +\n",
    "            r'([A-Z0-9\\s]{4})([0-9]{3})([\\s]{1})([0-9\\s]{3})([0-9\\s]{3})([0-9\\s]{2})([\\s]{1})' + \n",
    "            r'([0-9\\s]{2})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})([A-Z0-9\\s]{7})' + \n",
    "            r'([A-Z0-9\\s]{7})([\\s]{36})([A-Z0-9\\s]{2})([\\s]{1})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})' + \n",
    "            r'([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([A-Z0-9\\s]{5})([\\s]{1})([0-9\\s]{2})([0-9\\s]{1})' + \n",
    "            r'([0-9\\s]{1})([0-9\\s]{1})([0-9\\s]{1})([\\s]{33})([0-9\\s]{3})([0-9\\s]{1})([0-9\\s]{1})')\n",
    "    \n",
    "    #parsing starts. When parsing fails we just put a -99 there to indicated parsing failed in that row. \n",
    "    try:\n",
    "        rs = np.array(record_split.split(row))[selected_indices]\n",
    "    except:\n",
    "        rs = np.array(['-99'] * len(selected_indices))\n",
    "    return rs\n",
    "#     return record_split.split(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the record is parsed, we try to convert the list into a NumPy array and return it; \n",
    "if this fails we return a list of default values -99 so we know this record did not parse properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Defining pure python methods can slow down your application because Spark constantly needs to switch between Python interpreter and JVM. Whenver possible, we should you built-in python functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the `extractInformation(...)` method to split and convert our dataset.\n",
    "Note that we pass only the method signature to .map(...): the method will hand over one element of the RDD to the extractInformation(...) method at a time in each partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it is using lazy evaluztion ... so it is quick... because it doesn't do it yet...\n",
    "data_from_file_converted = data_from_file.map(extractInformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'2', u'00', u' ', u'0', u'02', u'F', u'4', u'001', u' ', u'03',\n",
       "        u'01', u'01', u'03', u'1', u'S', u'7', u'2014', u'U', u'7', u'U',\n",
       "        u'N', u' ', u' ', u'R571', u'380', u'110', u'136', u'36', u'01',\n",
       "        u'11R571 ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'01',\n",
       "        u'R571 ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'], \n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file_converted.map(lambda row: row).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing  variables to the workers\n",
    "\n",
    "when a job is submitted for execution, the job is sent to the driver (or a master) node. \n",
    "\n",
    "The driver node creates a DAGfor a job and decides which executor (or worker) nodes will run the specific tasks.\n",
    "\n",
    "The driver then instructs the workers to execute their tasks and return the results  to the driver when done. \n",
    "\n",
    "Each executor gets a copy of the variables and methods from the driver. If, when running the task, the executor alters these variables or overwrites the methods, it does so without affecting either other executors' copies or the variables and methods of the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .map(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is applied to each element of the RDD: in the case for the `data_from_file_conv` dataset you can think of this as a transformation of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014 = data_from_file_converted.map(lambda row: int(row[16]))\n",
    "data_2014.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2014', 2014, u'F', u' '),\n",
       " (u'2014', 2014, u'F', u' '),\n",
       " (u'2014', 2014, u'F', u' '),\n",
       " (u'2014', 2014, u'M', u' '),\n",
       " (u'2014', 2014, u'M', u' '),\n",
       " (u'2014', 2014, u'M', u' '),\n",
       " (u'2014', 2014, u'F', u' '),\n",
       " (u'2014', 2014, u'M', u' '),\n",
       " (u'2014', 2014, u'M', u' '),\n",
       " (u'2014', 2014, u'M', u' ')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2014_2 = data_from_file_converted.map(lambda row: (row[16], int(row[16]), row[5], row[21]))\n",
    "data_2014_2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .filter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.filter(...)` method allows you to select elements of your dataset that fit specified criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command might take a while depending on how fast your computer is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_from_file_converted.filter(lambda row: row[5] == 'F' and row[21] == '9')\n",
    "data_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'1', u'  ', u'2', u'1', u'03', u'F', u'1', u'037', u' ', u'33',\n",
       "        u'13', u'06', u'  ', u'2', u'D', u'3', u'2014', u'N', u'1', u'B',\n",
       "        u'Y', u'9', u'0', u'X44 ', u'420', u'122', u'   ', u'39', u'03',\n",
       "        u'11T509 ', u'12X44  ', u'61T509 ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'02',\n",
       "        u'X44  ', u'T509 ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'], \n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .flatMap(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.flatMap(...)` method works similarly to `.map(...)` but returns a flattened results instead of a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_filtered_flat = data_filtered.flatMap(lambda row: (row[16], int(row[16]) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered_flat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2014', 2015, u'2014', 2015, u'2014', 2015, u'2014', 2015]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered_flat.take(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are flattened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a list of distinct values in a specified column.\n",
    "It might take for a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file_converted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-99', u'M', u'F']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we will find the distinct values of column 5. \n",
    "distinct_gender = data_from_file_converted.map(lambda row: row[5]).distinct().collect()\n",
    "distinct_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .sample(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.sample()` method returns a randomized sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'1', u'06', u' ', u'0', u'06', u'F', u'1', u'065', u' ', u'39',\n",
       "        u'19', u'09', u'  ', u'2', u'M', u'5', u'2014', u'U', u'7', u'U',\n",
       "        u'N', u' ', u' ', u'E780', u'173', u'111', u'   ', u'37', u'03',\n",
       "        u'11I469 ', u'21I519 ', u'31E780 ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'03',\n",
       "        u'E780 ', u'I469 ', u'I519 ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'02', u' ', u' ', u'3', u'2', u'100', u'7'], \n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction = 0.1\n",
    "#False, fraction, 666 = With raplecement? Fraction of dataset used to sampling, random seed\n",
    "data_sample = data_from_file_converted.sample(False, fraction, 605)\n",
    "\n",
    "data_sample.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that we really got 1% of all the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 259, sample: 18\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset: {0}, sample: {1}'.format(data_from_file_converted.count(), data_sample.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .leftOuterJoin(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left outer join, just like the SQL world, joins two RDDs based on the values found in both datasets, and returns records from the left RDD with records from the right one appended where the two RDDs match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 4)),\n",
       " ('a', (1, 2)),\n",
       " ('a', (1, 1)),\n",
       " ('b', (4, '6')),\n",
       " (8, ('c', None))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 4), (8,'c')])\n",
    "rdd2 = sc.parallelize([('a', 4), ('a', 2), ('b', '6'), ('a',1), ('d', 15)])\n",
    "\n",
    "rdd3 = rdd1.leftOuterJoin(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d is missing since this is only a leftOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used `.join(...)` method instead we would have gotten only the values for `'a'` and `'b'` as these two values intersect between these two RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 4)), ('a', (1, 2)), ('a', (1, 1)), ('b', (4, '6'))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = rdd1.join(rdd2)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is the `.intersection(...)` that returns the records that are *equal* in both RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = rdd1.intersection(rdd2)\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .repartition(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartitioning the dataset changes the number of partitions the dataset is divided into.\n",
    "This functionality should be used sparingly and only when really \n",
    "necessary as it shuffles the data around, which in effect results in a significant  \n",
    "hit in terms of performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd1.repartition(6)\n",
    "\n",
    "len(rdd1.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [('b', 4)], [], [], [('a', 1), (8, 'c')], []]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .glom() method, in contrast to .collect(), produces a list where each element \n",
    "is another list of all elements of the dataset present in a specified partition; the main \n",
    "list returned has as many elements as the number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions, in contrast to transformations, execute the scheduled task on the \n",
    "dataset; once you have finished transforming your data you can execute your \n",
    "transformations. This might contain no transformations (for example, .take(n) will \n",
    "just return n records from an RDD even if you did not do any transformations to it) \n",
    "or execute the whole chain of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .take(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method returns `n` top rows from a single data partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is preferred to .collect(...) as it only returns the n top rows from a \n",
    "single data partition in contrast to .collect(...), which returns the whole RDD. \n",
    "This is especially important when you deal with large datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'2', u'00', u' ', u'0', u'02', u'F', u'4', u'001', u' ', u'03',\n",
       "        u'01', u'01', u'03', u'1', u'S', u'7', u'2014', u'U', u'7', u'U',\n",
       "        u'N', u' ', u' ', u'R571', u'380', u'110', u'136', u'36', u'01',\n",
       "        u'11R571 ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'01',\n",
       "        u'R571 ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'], \n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_first = data_from_file_converted.take(1)\n",
    "data_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want somewhat randomized records you can use .takeSample(...) \n",
    "instead, which takes three arguments: First whether the sampling should be with \n",
    "replacement, the second specifies the number of records to return, and the third  \n",
    "is a seed to the pseudo-random numbers generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_file_converted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([u'1', u'  ', u'2', u'1', u'10', u'F', u'1', u'064', u' ', u'38',\n",
       "        u'18', u'08', u'  ', u'4', u'M', u'6', u'2014', u'U', u'7', u'C',\n",
       "        u'N', u' ', u' ', u'C349', u'093', u'027', u'   ', u'08', u'02',\n",
       "        u'11C349 ', u'21F179 ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'02',\n",
       "        u'C349 ', u'F179 ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'01', u' ', u' ', u'1', u'1', u'100', u'6'], \n",
       "       dtype='<U40'),\n",
       " array([u'1', u'12', u' ', u'0', u'07', u'M', u'1', u'055', u' ', u'37',\n",
       "        u'17', u'08', u'  ', u'4', u'M', u'1', u'2014', u'U', u'7', u'U',\n",
       "        u'Y', u' ', u' ', u'I250', u'214', u'062', u'   ', u'21', u'02',\n",
       "        u'11I509 ', u'21I250 ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ',\n",
       "        u'       ', u'       ', u'       ', u'       ', u'       ', u'02',\n",
       "        u'I250 ', u'I509 ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'     ', u'     ', u'     ', u'     ',\n",
       "        u'     ', u'     ', u'02', u' ', u' ', u'3', u'2', u'100', u'7'], \n",
       "       dtype='<U40')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_take_sampled = data_from_file_converted.takeSample(False, 2, 667)\n",
    "data_take_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .collect(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns all the elements of the RDD to the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('a', 1), (8, 'c')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .reduce(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another action that processes your data, the `.reduce(...)` method *reduces* the elements of an RDD using a specified method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 4), ('c',8)])\n",
    "\n",
    "rdd1.map(lambda row: row[1]).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word of caution is necessary here. The functions passed as a reducer \n",
    "need to be associative, that is, when the order of elements is changed the \n",
    "result does not, and commutative, that is, changing the order of operands \n",
    "does not change the result either.\n",
    "The example of the associativity rule is (5 + 2) + 3 = 5 + (2 + 3), and of the \n",
    "commutative is 5 + 2 + 3 = 3 + 2 + 5. Thus, you need to be careful about \n",
    "what functions you pass to the reducer.\n",
    "\n",
    "If the reducing function is not associative and commutative you will sometimes get wrong results depending how your data is partitioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda row: row[1]).reduce(lambda x, y: x / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reduce = sc.parallelize([1.0, 2.0, .5, .1, 5, .2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 0.5, 0.1, 5, 0.2]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduce.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to reduce the data in a manner that we would like to *divide* the current result by the subsequent one, we would expect a value of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "works = data_reduce.reduce(lambda x, y: x / y)\n",
    "works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you were to partition the data into 3 partitions, the result will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduce = sc.parallelize([1.0, 2.0, .5, .1, 5, .2], 3)\n",
    "data_reduce.reduce(lambda x, y: x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.reduceByKey(...)` method works in a similar way to the `.reduce(...)` method but performs a reduction on a key-by-key basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 12), ('d', 5), ('c', 2), ('b', 4)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key = sc.parallelize([('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1),('d', 3)],4)\n",
    "data_key.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.count()` method counts the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduce.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .count(...) method produces the same result as the following method, but it \n",
    "does not require moving the whole dataset to the driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_reduce.collect()) # WRONG -- DON'T DO THIS! collect() moves the whole dataset to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset is in a form of a *key-value* you can use the `.countByKey()` method to get the counts of distinct keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 3), ('c', 2), ('a', 8), ('d', 2), ('b', 1), ('d', 3)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 2, 'c': 1, 'd': 2})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('c', 1), ('b', 2), ('d', 2)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.countByKey().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .saveAsTextFile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, the `.saveAsTextFile()` the RDD and saves it to text files: each partition to a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 4)], [('b', 3), ('c', 2)], [('a', 8), ('d', 2)], [('b', 1), ('d', 3)]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -r data_key.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure you delete the existing files before you run this cell!\n",
    "\n",
    "data_key.saveAsTextFile('data_key.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read it back, you need to parse it back as, as before, all the rows are treated as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 4), (u'b', 3), (u'c', 2), (u'a', 8), (u'd', 2), (u'b', 1), (u'd', 3)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseInput(row):\n",
    "    import re\n",
    "    \n",
    "    pattern = re.compile(r'\\(\\'([a-z])\\', ([0-9])\\)')\n",
    "    row_split = pattern.split(row)\n",
    "    \n",
    "    return (row_split[1], int(row_split[2]))\n",
    "    \n",
    "data_key_reread = sc \\\n",
    "    .textFile('data_key.txt') \\\n",
    "    .map(parseInput)\n",
    "data_key_reread.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.foreach(...)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method that applies the same function to each element of the RDD in an \n",
    "iterative way; in contrast to .map(..), the .foreach(...) method applies a defined \n",
    "function to each record in a one-by-one fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this prints to terminal not to jupyter notebook!\n",
    "def f(x): \n",
    "    print(x)\n",
    "    print('hi')\n",
    "\n",
    "data_key.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 3), ('c', 2), ('a', 8), ('d', 2), ('b', 1), ('d', 3)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
