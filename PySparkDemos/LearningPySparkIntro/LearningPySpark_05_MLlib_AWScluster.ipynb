{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Drabas & Lee  -- Learning PySpark\n",
    "## Resilient Distributed Datasets\n",
    "### The MLlib package\n",
    "#### Start the jupyter notebook from its own folder, otherwise python might not find some files to load!\n",
    "set the kernel to python 2 or Python [default]!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous chapter, we first specify the schema of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you only need to run this cell if the above spark context is not available when you start the notebook\n",
    "\n",
    "if 0:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    import pyspark\n",
    "\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql.session import SparkSession\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib stands for Machine Learning Library. \n",
    "\n",
    "MLlib is now in a maintenance mode, that is, it is not actively being developed (might be deprecated later)\n",
    "\n",
    "MLlib operatoes on RDDs.\n",
    "\n",
    "The documentation for MLlib can be found here: http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html\n",
    "\n",
    "Starting with Spark 2.0, ML is the main machine learning library that operates on DataFrames instead of RDDs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application:\n",
    "* Predict survival chances of infants using logistic regression\n",
    "* Select the most predictable features and train a random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the MLlib package\n",
    "\n",
    "At the high level, MLlib exposes three core machine learning functionalities:\n",
    "\n",
    "* **Data preparation**: Feature extraction, transformation, selection, hashing of categorical features, and some natural language processing methods\n",
    "* **Machine learning algorithms**: Some popular and advanced regression, classification, and clustering algorithms are implemented\n",
    "* **Utilities**: Statistical methods such as descriptive statistics, chi-square testing, \n",
    "linear algebra (sparse and dense matrices and vectors), and model evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a portion of the US 2014 and 2015 birth data we downloaded from http://www.cdc.gov/nchs/data_access/vitalstatsonline.htm; \n",
    "\n",
    "from the total of 300 variables we selected **85 features** that we will use to build our models. \n",
    "\n",
    "Also, out of the total of almost 7.99 million records, we selected a balanced sample of **45,429 records**: 22,080 records where infants were reported deceased and 23,349 records with infants alive.\n",
    "\n",
    "The dataset we will use in this chapter can be downloaded from http://www.tomdrabas.com/data/LearningPySpark/births_train.csv.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first specify the schema of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.StringType()),\n",
    "    ('BIRTH_YEAR', typ.IntegerType()),\n",
    "    ('BIRTH_MONTH', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.StringType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('MOTHER_RACE_6CODE', typ.StringType()),\n",
    "    ('MOTHER_EDUCATION', typ.StringType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('FATHER_EDUCATION', typ.StringType()),\n",
    "    ('MONTH_PRECARE_RECODE', typ.StringType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_BMI_RECODE', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.StringType()),\n",
    "    ('DIABETES_GEST', typ.StringType()),\n",
    "    ('HYP_TENS_PRE', typ.StringType()),\n",
    "    ('HYP_TENS_GEST', typ.StringType()),\n",
    "    ('PREV_BIRTH_PRETERM', typ.StringType()),\n",
    "    ('NO_RISK', typ.StringType()),\n",
    "    ('NO_INFECTIONS_REPORTED', typ.StringType()),\n",
    "    ('LABOR_IND', typ.StringType()),\n",
    "    ('LABOR_AUGM', typ.StringType()),\n",
    "    ('STEROIDS', typ.StringType()),\n",
    "    ('ANTIBIOTICS', typ.StringType()),\n",
    "    ('ANESTHESIA', typ.StringType()),\n",
    "    ('DELIV_METHOD_RECODE_COMB', typ.StringType()),\n",
    "    ('ATTENDANT_BIRTH', typ.StringType()),\n",
    "    ('APGAR_5', typ.IntegerType()),\n",
    "    ('APGAR_5_RECODE', typ.StringType()),\n",
    "    ('APGAR_10', typ.IntegerType()),\n",
    "    ('APGAR_10_RECODE', typ.StringType()),\n",
    "    ('INFANT_SEX', typ.StringType()),\n",
    "    ('OBSTETRIC_GESTATION_WEEKS', typ.IntegerType()),\n",
    "    ('INFANT_WEIGHT_GRAMS', typ.IntegerType()),\n",
    "    ('INFANT_ASSIST_VENTI', typ.StringType()),\n",
    "    ('INFANT_ASSIST_VENTI_6HRS', typ.StringType()),\n",
    "    ('INFANT_NICU_ADMISSION', typ.StringType()),\n",
    "    ('INFANT_SURFACANT', typ.StringType()),\n",
    "    ('INFANT_ANTIBIOTICS', typ.StringType()),\n",
    "    ('INFANT_SEIZURES', typ.StringType()),\n",
    "    ('INFANT_NO_ABNORMALITIES', typ.StringType()),\n",
    "    ('INFANT_ANCEPHALY', typ.StringType()),\n",
    "    ('INFANT_MENINGOMYELOCELE', typ.StringType()),\n",
    "    ('INFANT_LIMB_REDUCTION', typ.StringType()),\n",
    "    ('INFANT_DOWN_SYNDROME', typ.StringType()),\n",
    "    ('INFANT_SUSPECTED_CHROMOSOMAL_DISORDER', typ.StringType()),\n",
    "    ('INFANT_NO_CONGENITAL_ANOMALIES_CHECKED', typ.StringType()),\n",
    "    ('INFANT_BREASTFED', typ.StringType())\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = typ.StructType([\n",
    "        typ.StructField(e[0], e[1], False) for e in labels\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField(INFANT_ALIVE_AT_REPORT,StringType,false)\n",
      "StructField(BIRTH_YEAR,IntegerType,false)\n",
      "StructField(BIRTH_MONTH,IntegerType,false)\n",
      "StructField(BIRTH_PLACE,StringType,false)\n",
      "StructField(MOTHER_AGE_YEARS,IntegerType,false)\n",
      "StructField(MOTHER_RACE_6CODE,StringType,false)\n",
      "StructField(MOTHER_EDUCATION,StringType,false)\n",
      "StructField(FATHER_COMBINED_AGE,IntegerType,false)\n",
      "StructField(FATHER_EDUCATION,StringType,false)\n",
      "StructField(MONTH_PRECARE_RECODE,StringType,false)\n",
      "StructField(CIG_BEFORE,IntegerType,false)\n",
      "StructField(CIG_1_TRI,IntegerType,false)\n",
      "StructField(CIG_2_TRI,IntegerType,false)\n",
      "StructField(CIG_3_TRI,IntegerType,false)\n",
      "StructField(MOTHER_HEIGHT_IN,IntegerType,false)\n",
      "StructField(MOTHER_BMI_RECODE,IntegerType,false)\n",
      "StructField(MOTHER_PRE_WEIGHT,IntegerType,false)\n",
      "StructField(MOTHER_DELIVERY_WEIGHT,IntegerType,false)\n",
      "StructField(MOTHER_WEIGHT_GAIN,IntegerType,false)\n",
      "StructField(DIABETES_PRE,StringType,false)\n",
      "StructField(DIABETES_GEST,StringType,false)\n",
      "StructField(HYP_TENS_PRE,StringType,false)\n",
      "StructField(HYP_TENS_GEST,StringType,false)\n",
      "StructField(PREV_BIRTH_PRETERM,StringType,false)\n",
      "StructField(NO_RISK,StringType,false)\n",
      "StructField(NO_INFECTIONS_REPORTED,StringType,false)\n",
      "StructField(LABOR_IND,StringType,false)\n",
      "StructField(LABOR_AUGM,StringType,false)\n",
      "StructField(STEROIDS,StringType,false)\n",
      "StructField(ANTIBIOTICS,StringType,false)\n",
      "StructField(ANESTHESIA,StringType,false)\n",
      "StructField(DELIV_METHOD_RECODE_COMB,StringType,false)\n",
      "StructField(ATTENDANT_BIRTH,StringType,false)\n",
      "StructField(APGAR_5,IntegerType,false)\n",
      "StructField(APGAR_5_RECODE,StringType,false)\n",
      "StructField(APGAR_10,IntegerType,false)\n",
      "StructField(APGAR_10_RECODE,StringType,false)\n",
      "StructField(INFANT_SEX,StringType,false)\n",
      "StructField(OBSTETRIC_GESTATION_WEEKS,IntegerType,false)\n",
      "StructField(INFANT_WEIGHT_GRAMS,IntegerType,false)\n",
      "StructField(INFANT_ASSIST_VENTI,StringType,false)\n",
      "StructField(INFANT_ASSIST_VENTI_6HRS,StringType,false)\n",
      "StructField(INFANT_NICU_ADMISSION,StringType,false)\n",
      "StructField(INFANT_SURFACANT,StringType,false)\n",
      "StructField(INFANT_ANTIBIOTICS,StringType,false)\n",
      "StructField(INFANT_SEIZURES,StringType,false)\n",
      "StructField(INFANT_NO_ABNORMALITIES,StringType,false)\n",
      "StructField(INFANT_ANCEPHALY,StringType,false)\n",
      "StructField(INFANT_MENINGOMYELOCELE,StringType,false)\n",
      "StructField(INFANT_LIMB_REDUCTION,StringType,false)\n",
      "StructField(INFANT_DOWN_SYNDROME,StringType,false)\n",
      "StructField(INFANT_SUSPECTED_CHROMOSOMAL_DISORDER,StringType,false)\n",
      "StructField(INFANT_NO_CONGENITAL_ANOMALIES_CHECKED,StringType,false)\n",
      "StructField(INFANT_BREASTFED,StringType,false)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(schema)): print(schema[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though MLlib is designed with RDDs and DStreams in focus, for ease of transforming the data we will read the data and convert it to a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://ec2-18-223-209-87.us-east-2.compute.amazonaws.com:50070/fsck?ugi=ec2-user&path=%2Fhdfs_data%2Fbirths_train.csv.gz\r\n",
      "FSCK started by ec2-user (auth:SIMPLE) from /172.31.5.183 for path /hdfs_data/births_train.csv.gz at Mon Feb 11 05:35:06 UTC 2019\r\n",
      ".\r\n",
      "/hdfs_data/births_train.csv.gz:  Under replicated BP-663532545-172.31.27.125-1549216637007:blk_1073741838_1014. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s) and 0 decommissioning replica(s).\r\n",
      "Status: HEALTHY\r\n",
      " Total size:\t931988 B\r\n",
      " Total dirs:\t0\r\n",
      " Total files:\t1\r\n",
      " Total symlinks:\t\t0\r\n",
      " Total blocks (validated):\t1 (avg. block size 931988 B)\r\n",
      " Minimally replicated blocks:\t1 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t1 (100.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t3\r\n",
      " Average block replication:\t2.0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t1 (33.333332 %)\r\n",
      " Number of data-nodes:\t\t2\r\n",
      " Number of racks:\t\t1\r\n",
      "FSCK ended at Mon Feb 11 05:35:06 UTC 2019 in 1 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/hdfs_data/births_train.csv.gz' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -mkdir -p /hdfs_data\n",
    "#!hdfs dfs -ls /hdfs_data\n",
    "#!hdfs dfs -put data/births_train.csv.gz /hdfs_data\n",
    "!hdfs fsck /hdfs_data/births_train.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data. The .read.csv(...) method can read either uncompressed \n",
    "or (as in our case) GZipped comma-separated values. The header parameter set \n",
    "to True indicates that the first row contains the header, and we use the schema to \n",
    "specify the correct data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#births = spark.read.csv('data/births_train.csv.gz', header=True, schema=schema)\n",
    "births = spark.read.csv('/hdfs_data/births_train.csv.gz', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(INFANT_ALIVE_AT_REPORT=u'N', BIRTH_YEAR=2015, BIRTH_MONTH=2, BIRTH_PLACE=u'1', MOTHER_AGE_YEARS=29, MOTHER_RACE_6CODE=u'3', MOTHER_EDUCATION=u'9', FATHER_COMBINED_AGE=99, FATHER_EDUCATION=u'9', MONTH_PRECARE_RECODE=u'4', CIG_BEFORE=99, CIG_1_TRI=99, CIG_2_TRI=99, CIG_3_TRI=99, MOTHER_HEIGHT_IN=99, MOTHER_BMI_RECODE=9, MOTHER_PRE_WEIGHT=999, MOTHER_DELIVERY_WEIGHT=999, MOTHER_WEIGHT_GAIN=99, DIABETES_PRE=u'N', DIABETES_GEST=u'N', HYP_TENS_PRE=u'N', HYP_TENS_GEST=u'N', PREV_BIRTH_PRETERM=u'N', NO_RISK=u'1', NO_INFECTIONS_REPORTED=u'1', LABOR_IND=u'N', LABOR_AUGM=u'N', STEROIDS=u'N', ANTIBIOTICS=u'Y', ANESTHESIA=u'N', DELIV_METHOD_RECODE_COMB=u'2', ATTENDANT_BIRTH=u'1', APGAR_5=4, APGAR_5_RECODE=u'2', APGAR_10=3, APGAR_10_RECODE=u'1', INFANT_SEX=u'F', OBSTETRIC_GESTATION_WEEKS=35, INFANT_WEIGHT_GRAMS=2770, INFANT_ASSIST_VENTI=u'N', INFANT_ASSIST_VENTI_6HRS=u'N', INFANT_NICU_ADMISSION=u'Y', INFANT_SURFACANT=u'N', INFANT_ANTIBIOTICS=u'N', INFANT_SEIZURES=u'N', INFANT_NO_ABNORMALITIES=u'0', INFANT_ANCEPHALY=u'N', INFANT_MENINGOMYELOCELE=u'N', INFANT_LIMB_REDUCTION=u'N', INFANT_DOWN_SYNDROME=u'N', INFANT_SUSPECTED_CHROMOSOMAL_DISORDER=u'N', INFANT_NO_CONGENITAL_ANOMALIES_CHECKED=u'0', INFANT_BREASTFED=u'N')]\n"
     ]
    }
   ],
   "source": [
    "print(births.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of features in our dataset that are strings. These are mostly categorical variables that we need to somehow convert to a numeric form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify our recode dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this chapter is to predict whether the ``INFANT_ALIVE_AT_REPORT`` is \n",
    "either 1 or 0. Thus, we will drop all of the features that relate to the infant and will \n",
    "try to predict the infant's chances of surviving only based on the features related to \n",
    "its mother, father, and the place of birth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_BEFORE', \n",
    "    'CIG_1_TRI', \n",
    "    'CIG_2_TRI', \n",
    "    'CIG_3_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'MOTHER_DELIVERY_WEIGHT', \n",
    "    'MOTHER_WEIGHT_GAIN', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "\n",
    "births_trimmed = births.select(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=u'N', BIRTH_PLACE=u'1', MOTHER_AGE_YEARS=29, FATHER_COMBINED_AGE=99, CIG_BEFORE=99, CIG_1_TRI=99, CIG_2_TRI=99, CIG_3_TRI=99, MOTHER_HEIGHT_IN=99, MOTHER_PRE_WEIGHT=999, MOTHER_DELIVERY_WEIGHT=999, MOTHER_WEIGHT_GAIN=99, DIABETES_PRE=u'N', DIABETES_GEST=u'N', HYP_TENS_PRE=u'N', HYP_TENS_GEST=u'N', PREV_BIRTH_PRETERM=u'N')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births_trimmed.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, there are plenty of features with Yes/No/Unknown values; we will only code Yes to 1; everything else will be set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recode_dictionary = {\n",
    "    'YNU': {\n",
    "        'Y': 1,\n",
    "        'N': 0,\n",
    "        'U': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a small problem with how the number of cigarettes smoked by the mother was coded: as 0 means the mother smoked no cigarettes before or during the pregnancy, between 1-97 states the actual number of cigarette smoked, 98 indicates either 98 or more, whereas 99 identifies the unknown; we will assume the unknown is 0 and recode accordingly.\n",
    "\n",
    "Specify the recoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "def recode(col, key):        \n",
    "    return recode_dictionary[key][col] \n",
    "\n",
    "def correct_cig(feat):\n",
    "    return func \\\n",
    "        .when(func.col(feat) != 99, func.col(feat))\\\n",
    "        .otherwise(0)\n",
    "\n",
    "rec_integer = func.udf(recode, typ.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recode method looks up the correct key from the recode_dictionary (given \n",
    "the key) and returns the corrected value. \n",
    "\n",
    "The correct_cig method checks when the \n",
    "value of the feature feat is not equal to 99 and (for that situation) returns the value \n",
    "of the feature; if the value is equal to 99, we get 0 otherwise.\n",
    "\n",
    "\n",
    "We cannot use the recode function directly on a DataFrame; it needs to be converted \n",
    "to a UDF that Spark will understand. \n",
    "\n",
    "The rec_integer is such a function: by passing \n",
    "our specified recode function and specifying the return value data type, we can use \n",
    "it then to encode our Yes/No/Unknown features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll correct the features related to the number of cigarettes smoked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "births_transformed_tmp1 = births_trimmed \\\n",
    "    .withColumn('CIG_BEFORE', correct_cig('CIG_BEFORE'))\\\n",
    "    .withColumn('CIG_1_TRI', correct_cig('CIG_1_TRI'))\\\n",
    "    .withColumn('CIG_2_TRI', correct_cig('CIG_2_TRI'))\\\n",
    "    .withColumn('CIG_3_TRI', correct_cig('CIG_3_TRI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .withColumn(...) method takes the name of the column as its first parameter and the transformation as the second one. \n",
    "We do not create new columns, but reuse the same ones instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will focus on correcting the Yes/No/Unknown features. First, we will \n",
    "figure out which these are with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [(col.name, col.dataType) for col in births_trimmed.schema]\n",
    "\n",
    "YNU_cols = []\n",
    "\n",
    "for i, s in enumerate(cols):\n",
    "    if s[1] == typ.StringType():\n",
    "        dis = births.select(s[0]) \\\n",
    "            .distinct() \\\n",
    "            .rdd \\\n",
    "            .map(lambda row: row[0]) \\\n",
    "            .collect()\n",
    "\n",
    "        if 'Y' in dis:\n",
    "            YNU_cols.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we created a list of tuples (cols) that hold column names and corresponding data types. Next, we loop through all of these and calculate distinct values of all string columns; if a 'Y' is within the returned list, we append the column name to the YNU_cols list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INFANT_ALIVE_AT_REPORT',\n",
       " 'DIABETES_PRE',\n",
       " 'DIABETES_GEST',\n",
       " 'HYP_TENS_PRE',\n",
       " 'HYP_TENS_GEST',\n",
       " 'PREV_BIRTH_PRETERM']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YNU_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can transform the features *in bulk* while selecting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_NICU_ADMISSION=u'Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION=u'Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION=u'U', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION=u'N', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION=u'U', INFANT_NICU_ADMISSION_RECODE=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births.select([\n",
    "        'INFANT_NICU_ADMISSION', \n",
    "        rec_integer(\n",
    "            'INFANT_NICU_ADMISSION', func.lit('YNU')\n",
    "        ) \\\n",
    "        .alias('INFANT_NICU_ADMISSION_RECODE')]\n",
    "     ).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the 'INFANT_NICU_ADMISSION' column and we pass the name of the feature to the rec_integer method. \n",
    "\n",
    "We also alias the newly transformed column as 'INFANT_NICU_ADMISSION_RECODE'. \n",
    "\n",
    "This way we will also confirm that our UDF works as intended.\n",
    "\n",
    "So, to transform all the YNU_cols in one go, we will create a list of such transformations, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exprs_YNU = [\n",
    "    rec_integer(x, func.lit('YNU')).alias(x) \n",
    "    if x in YNU_cols \n",
    "    else x \n",
    "    for x in births_transformed_tmp1.columns\n",
    "]\n",
    "\n",
    "births_transformed_tmp2 = births_transformed_tmp1.select(exprs_YNU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we got it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-------------+------------------+\n",
      "|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "|           N|            N|           N|            N|                 N|\n",
      "|           N|            N|           N|            N|                 N|\n",
      "|           N|            N|           N|            N|                 N|\n",
      "|           N|            N|           N|            N|                 Y|\n",
      "|           N|            N|           N|            N|                 N|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.select(YNU_cols[-5:]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-------------+------------------+\n",
      "|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 1|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births_transformed_tmp2.select(YNU_cols[-5:]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed 'N' and 'U' got replaced to '0', and 'Y' got replaced to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get to know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will use the `colStats(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = ['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE',\n",
    "                'CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',\n",
    "                'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT',\n",
    "                'MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_rdd = births_transformed_tmp2\\\n",
    "                       .select(numeric_cols)\\\n",
    "                       .rdd \\\n",
    "                       .map(lambda row: [e for e in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mllib_stats = st.Statistics.colStats(numeric_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method takes an RDD of data to calculate the descriptive statistics of and return \n",
    "a MultivariateStatisticalSummary object that contains the following descriptive \n",
    "statistics:\n",
    "    \n",
    "* count(): This holds a row count\n",
    "* max(): This holds maximum value in the column\n",
    "* mean(): This holds the value of the mean for the values in the column\n",
    "* min(): This holds the minimum value in the column\n",
    "* normL1(): This holds the value of the L1-Norm for the values in the column\n",
    "* normL2(): This holds the value of the L2-Norm for the values in the column\n",
    "* numNonzeros(): This holds the number of nonzero values in the column\n",
    "* variance(): This holds the value of the variance for the values in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTHER_AGE_YEARS: \t28.30 \t 6.08\n",
      "FATHER_COMBINED_AGE: \t44.55 \t 27.55\n",
      "CIG_BEFORE: \t1.43 \t 5.18\n",
      "CIG_1_TRI: \t0.91 \t 3.83\n",
      "CIG_2_TRI: \t0.70 \t 3.31\n",
      "CIG_3_TRI: \t0.58 \t 3.11\n",
      "MOTHER_HEIGHT_IN: \t65.12 \t 6.45\n",
      "MOTHER_PRE_WEIGHT: \t214.50 \t 210.21\n",
      "MOTHER_DELIVERY_WEIGHT: \t223.63 \t 180.01\n",
      "MOTHER_WEIGHT_GAIN: \t30.74 \t 26.23\n"
     ]
    }
   ],
   "source": [
    "for col, m, v in zip(numeric_cols, \n",
    "                     mllib_stats.mean(), \n",
    "                     mllib_stats.variance()):\n",
    "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical variables we will calculate the frequencies of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('INFANT_ALIVE_AT_REPORT', [(1, 23349), (0, 22080)])\n",
      "('BIRTH_PLACE', [(u'1', 44558), (u'4', 327), (u'3', 224), (u'2', 136), (u'7', 91), (u'5', 74), (u'6', 11), (u'9', 8)])\n",
      "('DIABETES_PRE', [(0, 44881), (1, 548)])\n",
      "('DIABETES_GEST', [(0, 43451), (1, 1978)])\n",
      "('HYP_TENS_PRE', [(0, 44348), (1, 1081)])\n",
      "('HYP_TENS_GEST', [(0, 43302), (1, 2127)])\n",
      "('PREV_BIRTH_PRETERM', [(0, 43088), (1, 2341)])\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [e for e in births_transformed_tmp2.columns \n",
    "                    if e not in numeric_cols]\n",
    "\n",
    "categorical_rdd = births_transformed_tmp2\\\n",
    "                       .select(categorical_cols)\\\n",
    "                       .rdd \\\n",
    "                       .map(lambda row: [e for e in row])\n",
    "            \n",
    "for i, col in enumerate(categorical_cols):\n",
    "    agg = categorical_rdd \\\n",
    "        .groupBy(lambda row: row[i]) \\\n",
    "        .map(lambda row: (row[0], len(row[1])))\n",
    "        \n",
    "    print(col, sorted(agg.collect(), \n",
    "                      key=lambda el: el[1], \n",
    "                      reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the deliveries happened in hospital (BIRTH_PLACE equal to 1). Around 550 \n",
    "deliveries happened at home: some intentionally ('BIRTH_PLACE' equal to 3), and \n",
    "some not ('BIRTH_PLACE' equal to 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations help to identify collinear numeric features and handle them appropriately. \n",
    "Let's check the correlations between our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[29, 99, 0, 0, 0, 0, 99, 999, 999, 99],\n",
       " [22, 29, 0, 0, 0, 0, 65, 180, 198, 18],\n",
       " [38, 40, 0, 0, 0, 0, 63, 155, 167, 12]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrs = st.Statistics.corr(numeric_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIG_BEFORE-to-CIG_1_TRI: 0.83\n",
      "CIG_BEFORE-to-CIG_2_TRI: 0.72\n",
      "CIG_BEFORE-to-CIG_3_TRI: 0.62\n",
      "CIG_1_TRI-to-CIG_BEFORE: 0.83\n",
      "CIG_1_TRI-to-CIG_2_TRI: 0.87\n",
      "CIG_1_TRI-to-CIG_3_TRI: 0.76\n",
      "CIG_2_TRI-to-CIG_BEFORE: 0.72\n",
      "CIG_2_TRI-to-CIG_1_TRI: 0.87\n",
      "CIG_2_TRI-to-CIG_3_TRI: 0.89\n",
      "CIG_3_TRI-to-CIG_BEFORE: 0.62\n",
      "CIG_3_TRI-to-CIG_1_TRI: 0.76\n",
      "CIG_3_TRI-to-CIG_2_TRI: 0.89\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_DELIVERY_WEIGHT: 0.54\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.65\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_PRE_WEIGHT: 0.54\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.60\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_PRE_WEIGHT: 0.65\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_DELIVERY_WEIGHT: 0.60\n"
     ]
    }
   ],
   "source": [
    "for i, el in enumerate(corrs > 0.5):\n",
    "    correlated = [\n",
    "        (numeric_cols[j], corrs[i][j]) \n",
    "        for j, e in enumerate(el) \n",
    "        if e == 1.0 and j != i]\n",
    "    \n",
    "    if len(correlated) > 0:\n",
    "        for e in correlated:\n",
    "            print('{0}-to-{1}: {2:.2f}' \\\n",
    "                  .format(numeric_cols[i], e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code calculated the correlation matrix and print only those \n",
    "features that have a correlation coefficient greater than 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can drop most of highly correlated features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_keep = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_1_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "births_transformed = births_transformed_tmp2.select([e for e in features_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_keep = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'CIG_1_TRI', \n",
    "    ]\n",
    "births_transformed = births_transformed_tmp2.select([e for e in features_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot calculate correlations for the categorical features. However, we can run a \n",
    "Chi-square test to determine if there are significant differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Chi-square test to determine if there are significant differences for categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through all the categorical variables and pivot them by the 'INFANT_ALIVE_AT_REPORT' feature to get the counts. \n",
    "\n",
    "Next, we transform them into an RDD, so we can then convert them into a matrix using the pyspark.mllib.linalg module. \n",
    "\n",
    "The first parameter to the .Matrices.dense(...) method specifies the number of rows \n",
    "in the matrix; in our case, it is the length of distinct values of the categorical feature.\n",
    "The second parameter specifies the number of columns: we have two as our 'INFANT_ALIVE_AT_REPORT' target variable has only two values.\n",
    "\n",
    "The last parameter is a list of values to be transformed into a matrix.\n",
    "Here's an example that shows this more clearly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.linalg as ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1., 4.],\n",
      "             [2., 5.],\n",
      "             [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(ln.Matrices.dense(3,2, [1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BIRTH_PLACE', 0.0)\n",
      "('DIABETES_PRE', 0.0)\n",
      "('DIABETES_GEST', 0.0)\n",
      "('HYP_TENS_PRE', 0.0)\n",
      "('HYP_TENS_GEST', 0.0)\n",
      "('PREV_BIRTH_PRETERM', 0.0)\n"
     ]
    }
   ],
   "source": [
    "for cat in categorical_cols[1:]:\n",
    "    agg = births_transformed \\\n",
    "        .groupby('INFANT_ALIVE_AT_REPORT') \\\n",
    "        .pivot(cat) \\\n",
    "        .count()    \n",
    "\n",
    "    agg_rdd = agg \\\n",
    "        .rdd\\\n",
    "        .map(lambda row: (row[1:])) \\\n",
    "        .flatMap(lambda row: \n",
    "                 [0 if e == None else e for e in row]) \\\n",
    "        .collect()\n",
    "\n",
    "    row_length = len(agg.collect()[0]) - 1\n",
    "    agg = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    \n",
    "    test = st.Statistics.chiSqTest(agg)\n",
    "    print(cat, round(test.pValue, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tests reveal that all the features should be significantly different and should help \n",
    "us predict the chance of survival of an infant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is time to create our final dataset that we will use to build our models.\n",
    "\n",
    "We will convert our DataFrame into an RDD of LabeledPoints.\n",
    "\n",
    "A LabeledPoint is a MLlib structure that is used to train the machine learning \n",
    "models. It consists of two attributes: label and features.\n",
    "\n",
    "The label is our target variable and features can be a NumPy array, list, \n",
    "pyspark.mllib.linalg.SparseVector, pyspark.mllib.linalg.DenseVector, or \n",
    "scipy.sparse column matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD of `LabeledPoint`s\n",
    "\n",
    "\n",
    "Before we build our final dataset, we first need to deal with one final obstacle: our \n",
    "'BIRTH_PLACE' feature is still a string. While any of the other categorical variables \n",
    "can be used as is (as they are now dummy variables), we will use a hashing trick to \n",
    "encode the 'BIRTH_PLACE' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.regression as reg\n",
    "\n",
    "hashing = ft.HashingTF(7)\n",
    "\n",
    "births_hashed = births_transformed \\\n",
    "    .rdd \\\n",
    "    .map(lambda row: [\n",
    "            list(hashing.transform(row[1]).toArray()) \n",
    "                if col == 'BIRTH_PLACE' \n",
    "                else row[i] \n",
    "            for i, col \n",
    "            in enumerate(features_to_keep)]) \\\n",
    "    .map(lambda row: [[e] if type(e) == int else e \n",
    "                      for e in row]) \\\n",
    "    .map(lambda row: [item for sublist in row \n",
    "                      for item in sublist]) \\\n",
    "    .map(lambda row: reg.LabeledPoint(\n",
    "            row[0], \n",
    "            ln.Vectors.dense(row[1:]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,29.0,99.0,0.0,99.0,999.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,22.0,29.0,0.0,65.0,180.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,38.0,40.0,0.0,63.0,155.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,39.0,42.0,0.0,60.0,128.0,0.0,0.0,0.0,0.0,1.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,18.0,99.0,4.0,61.0,110.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,32.0,37.0,0.0,66.0,150.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,22.0,25.0,0.0,68.0,155.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,25.0,26.0,0.0,64.0,136.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,26.0,32.0,0.0,64.0,140.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,1.0,39.0,66.0,0.0,65.0,140.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births_hashed.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature has seven levels, so we use as many features as that for the hashing trick. Next, we actually use the model to convert our 'BIRTH_PLACE' feature into a SparseVector; such a data structure is preferred if your dataset has many columns but in a row only a few of them have non-zero values. We then combine all the features together and finally create a LabeledPoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before we move to the modeling stage, we need to split our dataset into two sets: one \n",
    "we'll use for training and the other for testing. Luckily, RDDs have a handy method \n",
    "to do just that: .randomSplit(...). The method takes a list of proportions that are \n",
    "to be used to randomly split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "births_train, births_test = births_hashed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting infant survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic regression in Spark\n",
    "\n",
    "MLLib used to provide a logistic regression model estimated using a stochastic gradient descent (SGD) algorithm. This model has been deprecated in Spark 2.0 in favor of the `LogisticRegressionWithLBFGS` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_Model = LogisticRegressionWithLBFGS.train(births_train, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LogisticRegressionWithLBFGS model uses the Limited-memory Broyden–\n",
    "Fletcher–Goldfarb–Shanno (BFGS) optimization algorithm. It is a quasi-Newton \n",
    "method that approximates the BFGS algorithm.\n",
    "\n",
    "Training the model is very simple: we just need to call the .train(...) method. \n",
    "The required parameters are the RDD with LabeledPoints; we also specified the \n",
    "number of iterations so it does not take too long to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the model to predict the classes for our testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction=LR_Model.predict(births_test.map(lambda row: row.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest in Spark\n",
    "\n",
    "We are now ready to build the random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "RF_model = RandomForest \\\n",
    "    .trainClassifier(data=births_train, \n",
    "                     numClasses=2, \n",
    "                     categoricalFeaturesInfo={}, \n",
    "                     numTrees=6,  \n",
    "                     featureSubsetStrategy='all',\n",
    "                     seed=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_prediction=RF_model.predict(births_test.map(lambda row: row.features))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_prediction.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
