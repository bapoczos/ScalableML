{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE5EXgN6bD-m",
        "outputId": "29cc3110-60b1-4bea-ccfb-de8b701488c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_mult.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matrix_mult.cu\n",
        "\n",
        "//Matrix multiplication using shared and non shared kernal\n",
        "#include \"stdio.h\"\n",
        "#include<iostream>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math.h>\n",
        "#define TILE_SIZE 2\n",
        "\n",
        "\n",
        "//Matrix multiplication using non shared kernel\n",
        "__global__ void gpu_Matrix_Mul_nonshared(float *d_a, float *d_b, float *d_c, const int size)\n",
        "{\n",
        "\tint row, col;\n",
        "\tcol = TILE_SIZE * blockIdx.x + threadIdx.x;\n",
        "\trow = TILE_SIZE * blockIdx.y + threadIdx.y;\n",
        "\n",
        "\tfor (int k = 0; k< size; k++)\n",
        "\t{\n",
        "\t\td_c[row*size + col] += d_a[row * size + k] * d_b[k * size + col];\n",
        "\t}\n",
        "}\n",
        "\n",
        "// Matrix multiplication using shared kernel\n",
        "__global__ void gpu_Matrix_Mul_shared(float *d_a, float *d_b, float *d_c, const int size)\n",
        "{\n",
        "\tint row, col;\n",
        "\t//Defining Shared Memory\n",
        "\t__shared__ float shared_a[TILE_SIZE][TILE_SIZE];\n",
        "\t__shared__ float shared_b[TILE_SIZE][TILE_SIZE];\n",
        "\tcol = TILE_SIZE * blockIdx.x + threadIdx.x;\n",
        "\trow = TILE_SIZE * blockIdx.y + threadIdx.y;\n",
        "\n",
        "\tfor (int i = 0; i< size / TILE_SIZE; i++) \n",
        "\t{\n",
        "\t\tshared_a[threadIdx.y][threadIdx.x] = d_a[row* size + (i*TILE_SIZE + threadIdx.x)];\n",
        "\t\tshared_b[threadIdx.y][threadIdx.x] = d_b[(i*TILE_SIZE + threadIdx.y) * size + col];\n",
        "\t\t__syncthreads(); \n",
        "\t\tfor (int j = 0; j<TILE_SIZE; j++)\n",
        "\t\t\td_c[row*size + col] += shared_a[threadIdx.y][j] * shared_b[j][threadIdx.x];\n",
        "\t\t__syncthreads(); \n",
        "\n",
        "\t}\n",
        "}\n",
        "\n",
        "// main routine\n",
        "int main()\n",
        "{\n",
        "\tconst int size = 4;\n",
        "\t//Define Host Array\n",
        "\tfloat h_a[size][size], h_b[size][size],h_result[size][size];\n",
        "\t//Defining device Array\n",
        "\tfloat *d_a, *d_b, *d_result; \n",
        "\t//Initialize host Array\n",
        "\tfor (int i = 0; i<size; i++)\n",
        "\t{\n",
        "\t\tfor (int j = 0; j<size; j++)\n",
        "\t\t{\n",
        "\t\t\th_a[i][j] = i;\n",
        "\t\t\th_b[i][j] = j;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tcudaMalloc((void **)&d_a, size*size*sizeof(int));\n",
        "\tcudaMalloc((void **)&d_b, size*size * sizeof(int));\n",
        "\tcudaMalloc((void **)&d_result, size*size* sizeof(int));\n",
        "\n",
        "\n",
        "\t//copy host array to device array\n",
        "\n",
        "\tcudaMemcpy(d_a, h_a, size*size* sizeof(int), cudaMemcpyHostToDevice);\n",
        "\tcudaMemcpy(d_b, h_b, size*size* sizeof(int), cudaMemcpyHostToDevice);\n",
        "\t\n",
        "\t//Define grid and block dimensions\n",
        "\tdim3 dimGrid(size / TILE_SIZE, size / TILE_SIZE, 1);\n",
        "\tdim3 dimBlock(TILE_SIZE, TILE_SIZE, 1);\n",
        "\t//gpu_Matrix_Mul_nonshared << <dimGrid, dimBlock >> > (d_a, d_b, d_result, size);\n",
        "\n",
        "\tgpu_Matrix_Mul_shared << <dimGrid, dimBlock >> > (d_a, d_b, d_result, size);\n",
        "\n",
        "\tcudaMemcpy(h_result, d_result, size*size * sizeof(int),\tcudaMemcpyDeviceToHost);\n",
        "\tprintf(\"The result of Matrix multiplication is: \\n\");\n",
        "\t\n",
        "\tfor (int i = 0; i< size; i++)\n",
        "\t{\n",
        "\t\tfor (int j = 0; j < size; j++)\n",
        "\t\t{\n",
        "\t\t\tprintf(\"%f   \", h_result[i][j]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_result);\n",
        "\treturn 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver -arch=sm_35 -Wno-deprecated-gpu-targets matrix_mult.cu"
      ],
      "metadata": {
        "id": "9xpr0GKcbU_6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L57p-Uvbbnl",
        "outputId": "aee452c4-b6e1-4e00-942a-67b7f6fd1038"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of Matrix multiplication is: \n",
            "0.000000   0.000000   0.000000   0.000000   \n",
            "0.000000   4.000000   8.000000   12.000000   \n",
            "0.000000   8.000000   16.000000   24.000000   \n",
            "0.000000   12.000000   24.000000   36.000000   \n"
          ]
        }
      ]
    }
  ]
}